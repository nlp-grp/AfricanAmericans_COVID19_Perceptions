{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Meghna/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import gzip\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 1013\n",
    "np.random.seed(SEED)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import pickle\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import sparse\n",
    "import os\n",
    "import pickle\n",
    "import emoji\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "from nltk.util import ngrams\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_files_path = r'/Users/Meghna/Desktop/covid_data/covid_tweets.json'\n",
    "\n",
    "covid_train_data = []\n",
    "noncovid_train_data = []\n",
    "covid_train_labels = []\n",
    "noncovid_train_labels = []\n",
    "\n",
    "covid_train_data2 = []\n",
    "noncovid_train_data2 = []\n",
    "\n",
    "\n",
    "try:\n",
    "    with open(covid_files_path,'r') as fin:\n",
    "        for line in fin:\n",
    "            tweet = json.loads(line)\n",
    "            covid_train_data.append(tweet['text'])\n",
    "            covid_train_labels.append(1)\n",
    "except:\n",
    "    print(\"Faulty file \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening file /Users/Meghna/Desktop/noncovid_data/noncovid_tweets_oct.gz\n",
      "opening file /Users/Meghna/Desktop/noncovid_data/noncovid_tweets_nov.gz\n"
     ]
    }
   ],
   "source": [
    "noncovid_files_path = r'/Users/Meghna/Desktop/noncovid_data/*'\n",
    "noncovid_files = glob.glob(noncovid_files_path)\n",
    "num_noncovid_tweets = 0\n",
    "\n",
    "for i in range(len(noncovid_files)):\n",
    "    print(\"opening file\", noncovid_files[i])\n",
    "    try:\n",
    "        with gzip.open(noncovid_files[i],'r') as fin:\n",
    "            for line in fin:\n",
    "                if num_noncovid_tweets < 27068:\n",
    "                    tweet = json.loads(line)\n",
    "                    noncovid_train_data.append(tweet['text'])\n",
    "                    noncovid_train_labels.append(0)\n",
    "                    num_noncovid_tweets += 1\n",
    "\n",
    "    except:\n",
    "        print(\"Faulty file \", noncovid_files[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "pat1 = r'@[A-Za-z0-9_]+'\n",
    "pat2 = r'https?://[^ ]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "www_pat = r'www.[^ ]+'\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "def tweet_cleaner_updated(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "    stripped = re.sub(combined_pat, '', bom_removed)\n",
    "    stripped = re.sub(www_pat, '', stripped)\n",
    "    stripped = re.sub(r'\\@w+','',stripped)\n",
    "    lower_case = stripped.lower()\n",
    "    lower_case = emoji.demojize(lower_case, language='en')\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]\n",
    "    return (\" \".join(words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:421: MarkupResemblesLocatorWarning: \"https://t.co/NSl7vPwMob\n",
      "SCUMBAG.\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n"
     ]
    }
   ],
   "source": [
    "for tweet in covid_train_data:\n",
    "    covid_train_data2.append(tweet_cleaner_updated(tweet))\n",
    "    \n",
    "for ntweet in noncovid_train_data:\n",
    "    noncovid_train_data2.append(tweet_cleaner_updated(ntweet))\n",
    "\n",
    "train_data = covid_train_data2 + noncovid_train_data2\n",
    "train_labels = covid_train_labels + noncovid_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus,test_corpus,train_labels,test_labels = train_test_split(train_data,train_labels,stratify=train_labels,test_size=0.25,random_state=1)\n",
    "train_texts, train_labels, test_texts, test_labels = train_corpus, train_labels, test_corpus, test_labels\n",
    "x_train, y_train, x_validation, y_validation = train_corpus,train_labels,test_corpus,test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def accuracy_summary3(pipeline, x_train, y_train, x_val,y_val,x_test, y_test):\n",
    "#     if y_val_len0 / (len(x_test)*1.) > 0.5:\n",
    "#         null_accuracy =  y_val_len1/ (len(x_test)*1.)\n",
    "#     else:\n",
    "#         null_accuracy = 1. - (y_val_len1 / (len(x_test)*1.))\n",
    "#     #t0 = time.time()\n",
    "#     sentiment_fit = pipeline.fit(x_train, y_train)\n",
    "#     pickle.dump(sentiment_fit, open('/Users/Meghna/Desktop/covid_noncovid_model.sav', 'wb'))\n",
    "#     #print(classification_report(y_true, y_pred))\n",
    "#     y_pred = sentiment_fit.predict(x_val)\n",
    "#     #train_test_time = time() - t0\n",
    "#     accuracy = accuracy_score(y_val, y_pred)\n",
    "    \n",
    "#     print (\"null accuracy: {0:.2f}%\".format(null_accuracy*100))\n",
    "#     print (\"accuracy score: {0:.2f}%\".format(accuracy*100))\n",
    "#     if accuracy > null_accuracy:\n",
    "#         print (\"model is {0:.2f}% more accurate than null accuracy\".format((accuracy-null_accuracy)*100))\n",
    "#     elif accuracy == null_accuracy:\n",
    "#         print (\"model has the same accuracy with the null accuracy\")\n",
    "#     else:\n",
    "#         print (\"model is {0:.2f}% less accurate than null accuracy\".format((null_accuracy-accuracy)*100))\n",
    "#     #print (\"train and test time: {0:.2f}s\".format(train_test_time))\n",
    "    \n",
    "#     y_true, y_pred_test = y_test, sentiment_fit.predict(x_test)\n",
    "#     print(classification_report(y_true, y_pred_test))\n",
    "#     print (\"-\"*80)\n",
    "#     return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result for Multinomial NB\n",
      "MultinomialNB()\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_test2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-6645b791e0a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation result for {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mclf_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_summary3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchecker_pipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x_test2' is not defined"
     ]
    }
   ],
   "source": [
    "# eclf6 = MultinomialNB()\n",
    "\n",
    "# for clf, label in zip([eclf6], ['Multinomial NB']):\n",
    "#     checker_pipeline = Pipeline([\n",
    "#             ('vectorizer', TfidfVectorizer(stop_words = 'english',max_features=30000,ngram_range=(1, 3))),\n",
    "#             ('classifier', clf)\n",
    "#         ])\n",
    "#     print (\"Validation result for {}\".format(label))\n",
    "#     print (clf)\n",
    "#     clf_accuracy = accuracy_summary3(checker_pipeline, x_train, y_train, x_validation, y_validation, x_test2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = TfidfVectorizer(stop_words = 'english',max_features=30000,ngram_range=(1, 3))\n",
    "features = t.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test features\n",
    "\n",
    "features_test = t.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(X_test, Y_test)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
