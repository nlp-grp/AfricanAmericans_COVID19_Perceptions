{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Meghna/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import gzip\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 1013\n",
    "np.random.seed(SEED)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import sparse\n",
    "import os\n",
    "import pickle\n",
    "import emoji\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "from nltk.util import ngrams\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import fbeta_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>races</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>And on the first day of the Colorado legislatu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Everyone needs to read this. Dr. Nichols and h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>“He would make outrageous claims like he inven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>@holynext SUGAR AND COVID\\nDiabetes is a targe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>I’m all for compromise - just do not give in o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37199</th>\n",
       "      <td>37199</td>\n",
       "      <td>1</td>\n",
       "      <td>There's great news about Covid vaccines, but w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37200</th>\n",
       "      <td>37200</td>\n",
       "      <td>1</td>\n",
       "      <td>@andybudd You're saying this like all travelli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37201</th>\n",
       "      <td>37201</td>\n",
       "      <td>1</td>\n",
       "      <td>Oh they was beaten or pepper sprayed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37202</th>\n",
       "      <td>37202</td>\n",
       "      <td>1</td>\n",
       "      <td>Maine CDC prepares to distribute COVID-19 vacc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37203</th>\n",
       "      <td>37203</td>\n",
       "      <td>1</td>\n",
       "      <td>@DearAuntCrabby Trump should spend a day at Fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37204 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  races                                             tweets\n",
       "0               0      0  And on the first day of the Colorado legislatu...\n",
       "1               1      1  Everyone needs to read this. Dr. Nichols and h...\n",
       "2               2      0  “He would make outrageous claims like he inven...\n",
       "3               3      0  @holynext SUGAR AND COVID\\nDiabetes is a targe...\n",
       "4               4      1  I’m all for compromise - just do not give in o...\n",
       "...           ...    ...                                                ...\n",
       "37199       37199      1  There's great news about Covid vaccines, but w...\n",
       "37200       37200      1  @andybudd You're saying this like all travelli...\n",
       "37201       37201      1               Oh they was beaten or pepper sprayed\n",
       "37202       37202      1  Maine CDC prepares to distribute COVID-19 vacc...\n",
       "37203       37203      1  @DearAuntCrabby Trump should spend a day at Fo...\n",
       "\n",
       "[37204 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "df=pd.read_csv('/Users/Meghna/Desktop/tweets_for_language_model_noRT.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18602\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>races</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>And on the first day of the Colorado legislatu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Everyone needs to read this. Dr. Nichols and h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>“He would make outrageous claims like he inven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>@holynext SUGAR AND COVID\\nDiabetes is a targe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>I’m all for compromise - just do not give in o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  races                                             tweets\n",
       "0           0      0  And on the first day of the Colorado legislatu...\n",
       "1           1      1  Everyone needs to read this. Dr. Nichols and h...\n",
       "2           2      0  “He would make outrageous claims like he inven...\n",
       "3           3      0  @holynext SUGAR AND COVID\\nDiabetes is a targe...\n",
       "4           4      1  I’m all for compromise - just do not give in o..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "races = df['races'].to_list()\n",
    "count = 0\n",
    "for val in races:\n",
    "    if val == 1:\n",
    "        count += 1\n",
    "print(count)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "pat1 = r'@[A-Za-z0-9_]+' #remove mentions\n",
    "pat2 = r'https?://[^ ]+' #remove hyperlinks\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "www_pat = r'www.[^ ]+'\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "def tweet_cleaner_updated(text):\n",
    "    soup = BeautifulSoup(text, 'lxml') # remove html tags\n",
    "    souped = soup.get_text()\n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\") #remove byte order marks\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    bom_removed = emoji.demojize(bom_removed, language='en')    \n",
    "    stripped = re.sub(combined_pat, '', bom_removed)\n",
    "    stripped = re.sub(www_pat, '', stripped)\n",
    "    stripped = re.sub(r'\\@w+','',stripped)\n",
    "    lower_case = stripped.lower()\n",
    "    lower_case = emoji.demojize(lower_case, language='en')\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled) #remove hashtag\n",
    "    tweets_clean = []\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]\n",
    "    for word in words:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation):  # remove punctuation\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "    return (\" \".join(tweets_clean)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df['tweets'].to_list()\n",
    "cleaned_tweets = []\n",
    "for tweet in tweets:\n",
    "    cleaned_tweets.append(tweet_cleaner_updated(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.DataFrame()\n",
    "df_new['cleaned_tweets'] = cleaned_tweets\n",
    "df_new['races'] = df['races']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /opt/anaconda3/lib/python3.7/site-packages (4.0.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/anaconda3/lib/python3.7/site-packages (from gensim) (1.19.5)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/lib/python3.7/site-packages (from gensim) (5.1.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/anaconda3/lib/python3.7/site-packages (from gensim) (1.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "races = df['races'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus,test_corpus,train_labels,test_labels = train_test_split(cleaned_tweets,races,stratify=races,test_size=0.25,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_newline(series):\n",
    "    return [review.replace('\\n','') for review in series]\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams(words, bi_min=15, tri_min=10):\n",
    "    bigram = gensim.models.Phrases(words, min_count = bi_min)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return bigram_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame()\n",
    "df_train['cleaned_tweets'] = train_corpus\n",
    "df_train['races'] = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(df_train):\n",
    "    \"\"\"\n",
    "    Get Bigram Model, Corpus, id2word mapping\n",
    "    \"\"\"\n",
    "    \n",
    "    df_train['cleaned_tweets'] = strip_newline(df_train.cleaned_tweets)\n",
    "    words = list(sent_to_words(df_train.cleaned_tweets))\n",
    "    bigram = bigrams(words)\n",
    "    bigram = [bigram[review] for review in words]\n",
    "#     lemma = lemmatization(bigram)\n",
    "    id2word = gensim.corpora.Dictionary(bigram)\n",
    "    id2word.filter_extremes(no_below=10, no_above=0.35)\n",
    "    id2word.compactify()\n",
    "    corpus = [id2word.doc2bow(text) for text in bigram]\n",
    "    return corpus, id2word, bigram\n",
    "\n",
    "train_corpus2, train_id2word, bigram_train = get_corpus(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['countri', 'led', 'women', 'cope', 'better', 'covid', 'via']\n"
     ]
    }
   ],
   "source": [
    "print(bigram_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging # This allows for seeing if the model converges. A log file is created.\n",
    "logging.basicConfig(filename='lda_model.log', format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    lda_train = gensim.models.ldamulticore.LdaMulticore(\n",
    "                           corpus=train_corpus2,\n",
    "                           num_topics=50,\n",
    "                           id2word=train_id2word,\n",
    "                           chunksize=100,\n",
    "                           workers=7, # Num. Processing Cores - 1\n",
    "                           passes=50,\n",
    "                           eval_every = 1,\n",
    "                           per_word_topics=True)\n",
    "    lda_train.save('lda_train.model')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(15,\n",
       "  '0.122*\"hospit\" + 0.049*\"differ\" + 0.042*\"learn\" + 0.029*\"record\" + 0.026*\"fear\" + 0.024*\"women\" + 0.023*\"healthcar\" + 0.022*\"access\" + 0.022*\"took\" + 0.022*\"went\" + 0.021*\"among\" + 0.020*\"patient\" + 0.019*\"aid\" + 0.017*\"cancer\" + 0.016*\"huge\"'),\n",
       " (43,\n",
       "  '0.166*\"right\" + 0.112*\"point\" + 0.046*\"backhand_index\" + 0.045*\"face_tear\" + 0.035*\"joy\" + 0.033*\"daili\" + 0.025*\"expos\" + 0.021*\"known\" + 0.021*\"review\" + 0.020*\"fauci\" + 0.016*\"joy_face\" + 0.015*\"tear_joy\" + 0.015*\"tori\" + 0.014*\"lie\" + 0.014*\"enforc\"'),\n",
       " (11,\n",
       "  '0.101*\"wear_mask\" + 0.063*\"care\" + 0.061*\"pleas\" + 0.037*\"everyon\" + 0.034*\"someon\" + 0.029*\"reason\" + 0.029*\"other\" + 0.029*\"might\" + 0.027*\"children\" + 0.027*\"protect\" + 0.025*\"take\" + 0.023*\"need\" + 0.023*\"one\" + 0.020*\"power\" + 0.018*\"fold_hand\"'),\n",
       " (42,\n",
       "  '0.095*\"countri\" + 0.089*\"world\" + 0.044*\"compani\" + 0.042*\"done\" + 0.036*\"someth\" + 0.035*\"heart\" + 0.026*\"would\" + 0.024*\"scientist\" + 0.021*\"name\" + 0.021*\"three\" + 0.021*\"smile_face\" + 0.021*\"pandem\" + 0.018*\"despit\" + 0.018*\"damag\" + 0.018*\"billion\"'),\n",
       " (38,\n",
       "  '0.122*\"vaccin\" + 0.096*\"uk\" + 0.056*\"approv\" + 0.031*\"lost\" + 0.029*\"eu\" + 0.029*\"brexit\" + 0.026*\"st\" + 0.025*\"roll\" + 0.023*\"author\" + 0.022*\"wrong\" + 0.019*\"shame\" + 0.019*\"regul\" + 0.018*\"us\" + 0.018*\"fast\" + 0.016*\"ahead\"'),\n",
       " (17,\n",
       "  '0.098*\"realli\" + 0.068*\"never\" + 0.061*\"actual\" + 0.049*\"best\" + 0.044*\"rule\" + 0.042*\"understand\" + 0.038*\"interest\" + 0.035*\"say\" + 0.035*\"better\" + 0.033*\"thread\" + 0.033*\"fight\" + 0.029*\"restaur\" + 0.028*\"stay_home\" + 0.023*\"hour\" + 0.023*\"social_distanc\"'),\n",
       " (41,\n",
       "  '0.108*\"need\" + 0.082*\"busi\" + 0.081*\"pandem\" + 0.063*\"respons\" + 0.039*\"servic\" + 0.032*\"offic\" + 0.032*\"rememb\" + 0.029*\"manag\" + 0.026*\"govt\" + 0.022*\"told\" + 0.022*\"cover\" + 0.018*\"season\" + 0.017*\"member\" + 0.016*\"essenti\" + 0.015*\"critic\"'),\n",
       " (37,\n",
       "  '0.162*\"day\" + 0.122*\"test\" + 0.073*\"back\" + 0.069*\"quarantin\" + 0.045*\"show\" + 0.036*\"posit\" + 0.035*\"work\" + 0.032*\"get\" + 0.030*\"week\" + 0.019*\"refus\" + 0.018*\"isol\" + 0.018*\"neg\" + 0.016*\"today\" + 0.015*\"time\" + 0.013*\"go_back\"'),\n",
       " (23,\n",
       "  '0.100*\"end\" + 0.064*\"bad\" + 0.045*\"staff\" + 0.034*\"shop\" + 0.034*\"tier\" + 0.029*\"total\" + 0.029*\"onlin\" + 0.027*\"limit\" + 0.025*\"secur\" + 0.022*\"announc\" + 0.019*\"deliv\" + 0.018*\"cnn\" + 0.018*\"capac\" + 0.017*\"twitter\" + 0.017*\"option\"'),\n",
       " (21,\n",
       "  '0.126*\"said\" + 0.041*\"level\" + 0.030*\"counti\" + 0.027*\"demand\" + 0.024*\"reach\" + 0.024*\"remind\" + 0.023*\"higher\" + 0.022*\"save_live\" + 0.021*\"pub\" + 0.019*\"respect\" + 0.019*\"deni\" + 0.017*\"doubt\" + 0.016*\"mother\" + 0.016*\"water\" + 0.015*\"precaut\"')]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_train.print_topics(20,num_words=15)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs = []\n",
    "for i in range(len(train_corpus)):\n",
    "    top_topics = lda_train.get_document_topics(train_corpus2[i], minimum_probability=0.0)\n",
    "    topic_vec = [top_topics[i][1] for i in range(50)]\n",
    "    train_vecs.append(topic_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.28856915,\n",
       " 0.0028573493,\n",
       " 0.14571416,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.14572206,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.1457059,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.0028573493,\n",
       " 0.14570802,\n",
       " 0.0028573493]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vecs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(train_vecs)\n",
    "y = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:195: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:195: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:195: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/opt/anaconda3/lib/python3.7/site-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/optimize.py:195: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Val f1: 0.534 +- 0.004\n",
      "Logisitic Regression SGD Val f1: 0.537 +- 0.023\n",
      "SVM Huber Val f1: 0.399 +- 0.326\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(5, shuffle=True, random_state=42)\n",
    "cv_lr_f1, cv_lrsgd_f1, cv_svcsgd_f1,  = [], [], []\n",
    "\n",
    "for train_ind, val_ind in kf.split(X, y):\n",
    "    # Assign CV IDX\n",
    "    X_train, y_train = X[train_ind], y[train_ind]\n",
    "    X_val, y_val = X[val_ind], y[val_ind]\n",
    "    \n",
    "    # Scale Data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scale = scaler.fit_transform(X_train)\n",
    "    X_val_scale = scaler.transform(X_val)\n",
    "\n",
    "    # Logisitic Regression\n",
    "    lr = LogisticRegression(\n",
    "        class_weight= 'balanced',\n",
    "        solver='newton-cg',\n",
    "        fit_intercept=True\n",
    "    ).fit(X_train_scale, y_train)\n",
    "\n",
    "    y_pred = lr.predict(X_val_scale)\n",
    "    cv_lr_f1.append(f1_score(y_val, y_pred, average='binary'))\n",
    "    \n",
    "    # Logistic Regression Mini-Batch SGD\n",
    "    sgd = linear_model.SGDClassifier(\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "        loss='log',\n",
    "        class_weight='balanced'\n",
    "    ).fit(X_train_scale, y_train)\n",
    "    \n",
    "    y_pred = sgd.predict(X_val_scale)\n",
    "    cv_lrsgd_f1.append(f1_score(y_val, y_pred, average='binary'))\n",
    "    \n",
    "    # SGD Modified Huber\n",
    "    sgd_huber = linear_model.SGDClassifier(\n",
    "        max_iter=1000,\n",
    "        tol=1e-3,\n",
    "        alpha=20,\n",
    "        loss='modified_huber',\n",
    "        class_weight='balanced'\n",
    "    ).fit(X_train_scale, y_train)\n",
    "    \n",
    "    y_pred = sgd_huber.predict(X_val_scale)\n",
    "    cv_svcsgd_f1.append(f1_score(y_val, y_pred, average='binary'))\n",
    "\n",
    "print(f'Logistic Regression Val f1: {np.mean(cv_lr_f1):.3f} +- {np.std(cv_lr_f1):.3f}')\n",
    "print(f'Logisitic Regression SGD Val f1: {np.mean(cv_lrsgd_f1):.3f} +- {np.std(cv_lrsgd_f1):.3f}')\n",
    "print(f'SVM Huber Val f1: {np.mean(cv_svcsgd_f1):.3f} +- {np.std(cv_svcsgd_f1):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
